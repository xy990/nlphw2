{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import csv\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "# Fasttext\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "def build_vocab():\n",
    "    words_to_load = 50000\n",
    "\n",
    "    with open('wiki-news-300d-1M.vec', 'rb') as f:\n",
    "        loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "        words2idx_ft = {}\n",
    "        idx2words_ft = {}\n",
    "        ordered_words_ft = []\n",
    "        #give <pad> and <unk> random vectors\n",
    "        loaded_embeddings_ft[0] = np.random.rand(1,300) * 0\n",
    "        loaded_embeddings_ft[1] = np.random.rand(1,300)\n",
    "\n",
    "        idx2words_ft[PAD_IDX] =  '<pad>'\n",
    "        idx2words_ft[UNK_IDX] = '<unk>'\n",
    "        words2idx_ft['<pad>'] = PAD_IDX\n",
    "        words2idx_ft['<unk>'] = UNK_IDX\n",
    "        ordered_words_ft.append('<pad>')\n",
    "        ordered_words_ft.append('<unk>')\n",
    "\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.decode('utf8').split()\n",
    "            loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "            words2idx_ft[s[0]] = i+2\n",
    "            idx2words_ft[i+2] = s[0]\n",
    "            ordered_words_ft.append(s[0])\n",
    "        return words2idx_ft, idx2words_ft, loaded_embeddings_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "def convert_to_words(data):\n",
    "    return [(sample[0].split(), sample[1].split(), sample[2]) for sample in data]\n",
    "\n",
    "def read_data():\n",
    "    train_dataset = 'snli_train.tsv'\n",
    "    val_dataset = 'snli_val.tsv'\n",
    "    with open(train_dataset) as tsvfile1:\n",
    "        reader1 = csv.reader(tsvfile1, delimiter='\\t')\n",
    "        train_data =convert_to_words(reader1)\n",
    "    max_len1 = max([len(word[0]) for word in train_data])\n",
    "    max_len2 = max([len(word[0]) for word in train_data])\n",
    "    max_len = max(max_len1, max_len2)\n",
    "    \n",
    "    with open(val_dataset) as tsvfile2:\n",
    "        reader2 = csv.reader(tsvfile2, delimiter = '\\t')\n",
    "        val_data = convert_to_words(reader2)\n",
    "    words2id, id2words, loaded_embeddings = build_vocab()\n",
    "    return train_data[1:10000], val_data[1:10000], words2id, id2words, loaded_embeddings, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = build_vocab()[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum word length of dataset is 82\n",
      "Number of words in dataset is 50002\n"
     ]
    }
   ],
   "source": [
    "# In[5]:\n",
    "\n",
    "train_data, val_data, words2id, id2words, loaded_embeddings, max_len =read_data()\n",
    "labels_list = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "print (\"Maximum word length of dataset is {}\".format(max_len))\n",
    "print (\"Number of words in dataset is {}\".format(len(id2words)))\n",
    "#print (\"Characters:\")\n",
    "#print (char2id.keys())\n",
    "#print(train_data[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    index_lists1 = []\n",
    "    index_lists2 = []\n",
    "    index_lists3 = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list1 = [words2id[token] if token in words2id else UNK_IDX for token in tokens[0]]\n",
    "        index_list2 = [words2id[token] if token in words2id else UNK_IDX for token in tokens[1]]\n",
    "        index_lists3.append(tokens[2])\n",
    "        index_lists1.append(index_list1)\n",
    "        index_lists2.append(index_list2)\n",
    "    return index_lists1,index_lists2, index_lists3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data_indices1, train_data_indices2, train_target_indices = token2index_dataset(train_data)\n",
    "val_data_indices1, val_data_indices2, val_target_indices = token2index_dataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, words2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1,self.data_list2, self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) ==len(self.data_list2))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "        self.words2id = words2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        word_idx1 = [self.words2id[c] if c in self.words2id else UNK_IDX  for c in self.data_list1[key][:max_len]]\n",
    "        word_idx2 = [self.words2id[c] if c in self.words2id else UNK_IDX  for c in self.data_list2[key][:max_len]]\n",
    "        label = labels_list.index(self.target_list[key])\n",
    "        return [word_idx1,word_idx2, len(word_idx1),len(word_idx2), label]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,max_len -datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "        padded_vec2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0, max_len -datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "\n",
    "    \n",
    "\n",
    "    return (torch.LongTensor(data_list1).cuda(), \n",
    "        torch.LongTensor(data_list2).cuda(),\n",
    "        torch.LongTensor(length_list1).cuda(), \n",
    "        torch.LongTensor(length_list2).cuda(),\n",
    "        torch.LongTensor(label_list).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "train_dataset = VocabDataset(train_data, words2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_data, words2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "#CNN\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings), freeze=True).cuda()\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        \n",
    "        #self.embedding.weight.data.copy_(torch.from_numpy(build_vocab()[2]))\n",
    "        #self.embedding.weight.requires_grad = False\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(82, stride = 2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2,64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "\n",
    "        #self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x1,x2, lengths1, lengths2):\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "        x1 = x1.long()\n",
    "\n",
    "        embed1 = self.embedding(x1).float()\n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        #print(hidden1.size())\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, seq_len1, hidden1.size(-1))\n",
    "        #print(hidden1.size())\n",
    "\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        #print(hidden1.size())\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, seq_len1, hidden1.size(-1))\n",
    "        #print(hidden1.size())\n",
    "        \n",
    "        output1 = self.maxpool(hidden1.transpose(1,2)).transpose(1, 2).squeeze(1)\n",
    "        #print(hidden1.size())\n",
    "        #output1 = torch.max(output1, dim=1)[0]\n",
    "        \n",
    "        \n",
    "        batch_size2, seq_len2 = x1.size()\n",
    "        x2 = x2.long()\n",
    "\n",
    "        embed2 = self.embedding(x2)\n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "        #print(hidden2.transpose(1,2).size())\n",
    "        output2 = self.maxpool(hidden2.transpose(1,2)).transpose(1, 2).squeeze(1)\n",
    "        #print(output2.size())\n",
    "        \n",
    "        #output2 = torch.max(output2, dim=1)[0]\n",
    "        #print(output2.size())\n",
    "        output = torch.cat((output1, output2), dim=1)\n",
    "        #print(output.size())\n",
    "        output = F.relu(self.fc1(output))\n",
    "        #print(output.size())\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        output1 = torch.max(hidden1, dim=1)[0]\n",
    "        #print(hidden1.size())\n",
    "        #output1 = torch.sum(output1, dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x2 = x2.long()\n",
    "\n",
    "        embed2 = self.embedding(x2).float()\n",
    "        hidden2 = self.conv1(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "        output2 = torch.max(hidden1, dim=1)[0]\n",
    "        \n",
    "        #output2 = torch.sum(output2, dim=1)\n",
    "        output = torch.cat((output1, output2), dim=1)\n",
    "    \n",
    "        #print(output.size())\n",
    "        output = F.relu(self.fc1(output))\n",
    "        #print(output.size())\n",
    "        output = self.fc2(output)\n",
    "        #print(output.size())\n",
    "        #output = F.softmax(output, dim =1)\n",
    "        #print(output.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #hidden = torch.sum(hidden, dim=1)\n",
    "        #logits = self.linear(hidden)\n",
    "        '''\n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1,data2, lengths1, lengths2, labels in loader:\n",
    "        data_batch1,data_batch2, lengths_batch1,lengths_batch2, label_batch = data1,data2, lengths1, lengths2,labels\n",
    "        outputs = F.softmax(model(data_batch1,data_batch2, lengths_batch1, lengths_batch2), dim=1)\n",
    "        #outputs = model(data_batch1,data_batch2, lengths_batch1, lengths_batch2)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def earily_stop(val_acc_history, t=5, required_progress=0.001):\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_acc_history: a list contains all the historical validation acc\n",
    "    @param required_progress: the next acc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should earily stop\n",
    "    \"\"\"\n",
    "    # TODO: Finished\n",
    "    stop=False\n",
    "    repeat=0\n",
    "    if len(val_acc_history)<=t:\n",
    "        stop=False\n",
    "    else:\n",
    "        for i in range(1,t+1):\n",
    "            i=-i\n",
    "            diff=val_acc_history[i]-val_acc_history[i-1]\n",
    "            \n",
    "            if diff-required_progress<=0.00001:\n",
    "                repeat+=1\n",
    "            if repeat==t:\n",
    "                stop=True\n",
    "                break\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/313],  training loss: 11.1007155418396, Train Acc: 41.964196419641965, Validation Acc: 38.6\n",
      "Epoch: [1/10], Step: [201/313],  training loss: 10.94191154241562, Train Acc: 37.85378537853786, Validation Acc: 36.2\n",
      "Epoch: [1/10], Step: [301/313],  training loss: 10.900769138336182, Train Acc: 47.914791479147915, Validation Acc: 45.3\n",
      "Epoch: [2/10], Step: [101/313],  training loss: 10.84502398967743, Train Acc: 49.104910491049104, Validation Acc: 48.5\n",
      "Epoch: [2/10], Step: [201/313],  training loss: 10.393507212400436, Train Acc: 51.23512351235124, Validation Acc: 48.0\n",
      "Epoch: [2/10], Step: [301/313],  training loss: 10.155423486232758, Train Acc: 55.105510551055104, Validation Acc: 51.6\n",
      "Epoch: [3/10], Step: [101/313],  training loss: 9.699889653921128, Train Acc: 56.70567056705671, Validation Acc: 52.8\n",
      "Epoch: [3/10], Step: [201/313],  training loss: 9.31410629749298, Train Acc: 56.37563756375638, Validation Acc: 54.5\n",
      "Epoch: [3/10], Step: [301/313],  training loss: 9.340422052145005, Train Acc: 58.11581158115811, Validation Acc: 54.5\n",
      "Epoch: [4/10], Step: [101/313],  training loss: 9.144377249479295, Train Acc: 59.185918591859185, Validation Acc: 55.1\n",
      "Epoch: [4/10], Step: [201/313],  training loss: 8.895083832740784, Train Acc: 59.54595459545955, Validation Acc: 56.1\n",
      "Epoch: [4/10], Step: [301/313],  training loss: 8.948699402809144, Train Acc: 60.17601760176018, Validation Acc: 55.8\n",
      "Epoch: [5/10], Step: [101/313],  training loss: 8.715113788843155, Train Acc: 61.33613361336133, Validation Acc: 56.5\n",
      "Epoch: [5/10], Step: [201/313],  training loss: 8.562696558237075, Train Acc: 61.75617561756176, Validation Acc: 57.0\n",
      "Epoch: [5/10], Step: [301/313],  training loss: 8.799980729818344, Train Acc: 60.50605060506051, Validation Acc: 56.1\n",
      "Epoch: [6/10], Step: [101/313],  training loss: 8.650360590219497, Train Acc: 62.206220622062204, Validation Acc: 56.7\n",
      "Epoch: [6/10], Step: [201/313],  training loss: 8.566096568107605, Train Acc: 63.216321632163215, Validation Acc: 57.6\n",
      "Epoch: [6/10], Step: [301/313],  training loss: 8.241901016235351, Train Acc: 63.546354635463544, Validation Acc: 58.2\n",
      "Epoch: [7/10], Step: [101/313],  training loss: 8.479824405908584, Train Acc: 63.1963196319632, Validation Acc: 56.1\n",
      "Epoch: [7/10], Step: [201/313],  training loss: 8.23577241897583, Train Acc: 62.536253625362534, Validation Acc: 58.2\n",
      "Epoch: [7/10], Step: [301/313],  training loss: 8.254624420404435, Train Acc: 63.316331633163315, Validation Acc: 56.4\n",
      "Epoch: [8/10], Step: [101/313],  training loss: 7.965094745159149, Train Acc: 64.82648264826483, Validation Acc: 56.9\n",
      "Epoch: [8/10], Step: [201/313],  training loss: 8.332603389024735, Train Acc: 64.6064606460646, Validation Acc: 58.0\n",
      "Epoch: [8/10], Step: [301/313],  training loss: 8.073732978105545, Train Acc: 65.50655065506551, Validation Acc: 57.8\n",
      "Epoch: [9/10], Step: [101/313],  training loss: 8.024418836832046, Train Acc: 66.57665766576658, Validation Acc: 57.3\n",
      "Epoch: [9/10], Step: [201/313],  training loss: 7.8937999427318575, Train Acc: 66.98669866986698, Validation Acc: 58.0\n",
      "Epoch: [9/10], Step: [301/313],  training loss: 7.855227476358413, Train Acc: 67.21672167216721, Validation Acc: 57.6\n",
      "Epoch: [10/10], Step: [101/313],  training loss: 7.658551019430161, Train Acc: 67.04670467046705, Validation Acc: 57.8\n",
      "Epoch: [10/10], Step: [201/313],  training loss: 7.7964017450809475, Train Acc: 66.72667266726673, Validation Acc: 58.1\n",
      "Epoch: [10/10], Step: [301/313],  training loss: 7.850267249345779, Train Acc: 67.53675367536754, Validation Acc: 59.2\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=200, num_layers=2, vocab_size=len(id2words)).cuda()\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "validation_acc_history = []\n",
    "stop_training = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (data1,data2, lengths1,lengths2, labels) in enumerate(train_loader):\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1,data2, lengths1,lengths2)\n",
    "        #print(outputs.size())\n",
    "        #print(labels.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}],  training loss: {}, Train Acc: {}, Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader),  running_loss/10, train_acc, val_acc))\n",
    "            running_loss = 0.0\n",
    "            validation_acc_history.append(val_acc)\n",
    "            # check if we need to earily stop the model\n",
    "            stop_training = earily_stop(validation_acc_history)\n",
    "            \n",
    "            if stop_training:\n",
    "                print(\"earily stop triggered\")\n",
    "                break\n",
    "    # because of the the nested loop\n",
    "    if stop_training:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "#Bidirectional GRU\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, linear_size):\n",
    "    \n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding.from_pretrained(embedding_weight_matrix, freeze=True)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        \n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(build_vocab()[2]))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.gru= nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_1 = nn.Linear(hidden_size*2, linear_size)\n",
    "        self.linear_2 = nn.Linear(linear_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.randn(self.num_layers * 2, batch_size, self.hidden_size).cuda()\n",
    "        '''\n",
    "        \n",
    "        if torch.cuda.is_available and torch.has_cudnn:\n",
    "            hidden = torch.randn(self.num_layers * 2, batch_size, self.hidden_size).cuda()\n",
    "        else:\n",
    "            hidden = torch.randn(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "        '''\n",
    "        return hidden\n",
    "    \n",
    "\n",
    "    def forward(self, premise, hypothesis, pre_length, hyp_length):\n",
    "        # reset hidden state\n",
    "        batch_size, seq_len_premise = premise.size()\n",
    "        batch_size, seq_len_hypothesis = hypothesis.size()\n",
    "\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "         # compute sorted sequence lengths\n",
    "        _, idx_sort_1 = torch.sort(pre_length, dim=0, descending=True)\n",
    "        _, idx_sort_2 = torch.sort(hyp_length, dim=0, descending=True)\n",
    "        _, idx_unsort_1 = torch.sort(idx_sort_1, dim=0)\n",
    "        _, idx_unsort_2 = torch.sort(idx_sort_2, dim=0)\n",
    "        \n",
    "        # get embedding of characters\n",
    "        embed_1 = self.embedding(premise).float()\n",
    "        embed_2 = self.embedding(hypothesis).float()\n",
    "        # sort embeddings and lengths\n",
    "        embed_1 = embed_1.index_select(0,idx_sort_1)\n",
    "        embed_2 = embed_2.index_select(0,idx_sort_2)\n",
    "        len1 = list(pre_length[idx_sort_1])\n",
    "        len2 = list(hyp_length[idx_sort_2])\n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed_1 = torch.nn.utils.rnn.pack_padded_sequence(embed_1, np.array(len1), batch_first=True)\n",
    "        embed_2 = torch.nn.utils.rnn.pack_padded_sequence(embed_2, np.array(len2), batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        _, hidden_1 = self.gru(embed_1, self.hidden)\n",
    "        _, hidden_2 = self.gru(embed_2, self.hidden)\n",
    "        \n",
    "        # sum the hidden state on the first dimension\n",
    "        hidden_1 = torch.sum(hidden_1, dim=0)\n",
    "        hidden_2 = torch.sum(hidden_2, dim=0)\n",
    "        \n",
    "        # unsort the hidden state and concatenate the two\n",
    "        hidden_1 = hidden_1.index_select(0, idx_unsort_1)\n",
    "        hidden_2 = hidden_2.index_select(0, idx_unsort_2)\n",
    "        \n",
    "        concat_input = torch.cat((hidden_1, hidden_2), dim=1)\n",
    "        output = self.linear_1(concat_input)\n",
    "        output = self.relu(output)\n",
    "        logits = self.linear_2(output)\n",
    "        return logits\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/313],  training loss: 11.065701937675476, Train Acc: 45.53455345534554, Validation Acc: 43.0\n",
      "Epoch: [1/10], Step: [201/313],  training loss: 10.72524288892746, Train Acc: 46.83468346834683, Validation Acc: 45.4\n",
      "Epoch: [1/10], Step: [301/313],  training loss: 10.43424677848816, Train Acc: 52.045204520452046, Validation Acc: 50.7\n",
      "Epoch: [2/10], Step: [101/313],  training loss: 10.10987230539322, Train Acc: 52.34523452345235, Validation Acc: 53.3\n",
      "Epoch: [2/10], Step: [201/313],  training loss: 9.699177259206772, Train Acc: 55.685568556855685, Validation Acc: 54.9\n",
      "Epoch: [2/10], Step: [301/313],  training loss: 9.536523520946503, Train Acc: 56.925692569256924, Validation Acc: 55.8\n",
      "Epoch: [3/10], Step: [101/313],  training loss: 9.310713976621628, Train Acc: 56.975697569756974, Validation Acc: 55.8\n",
      "Epoch: [3/10], Step: [201/313],  training loss: 9.242854982614517, Train Acc: 58.58585858585859, Validation Acc: 56.8\n",
      "Epoch: [3/10], Step: [301/313],  training loss: 9.191554707288741, Train Acc: 58.24582458245825, Validation Acc: 55.5\n",
      "Epoch: [4/10], Step: [101/313],  training loss: 9.070059269666672, Train Acc: 60.12601260126013, Validation Acc: 55.8\n",
      "Epoch: [4/10], Step: [201/313],  training loss: 8.972361505031586, Train Acc: 59.615961596159615, Validation Acc: 56.2\n",
      "Epoch: [4/10], Step: [301/313],  training loss: 8.973931932449341, Train Acc: 59.72597259725973, Validation Acc: 56.8\n",
      "Epoch: [5/10], Step: [101/313],  training loss: 8.806751269102097, Train Acc: 60.45604560456046, Validation Acc: 57.2\n",
      "Epoch: [5/10], Step: [201/313],  training loss: 8.814041900634766, Train Acc: 60.83608360836084, Validation Acc: 57.8\n",
      "Epoch: [5/10], Step: [301/313],  training loss: 8.684706425666809, Train Acc: 61.536153615361535, Validation Acc: 58.2\n",
      "Epoch: [6/10], Step: [101/313],  training loss: 8.55076680779457, Train Acc: 62.156215621562154, Validation Acc: 56.6\n",
      "Epoch: [6/10], Step: [201/313],  training loss: 8.713964104652405, Train Acc: 61.94619461946195, Validation Acc: 55.7\n",
      "Epoch: [6/10], Step: [301/313],  training loss: 8.54714829325676, Train Acc: 62.66626662666267, Validation Acc: 58.8\n",
      "Epoch: [7/10], Step: [101/313],  training loss: 8.501535379886628, Train Acc: 63.14631463146315, Validation Acc: 56.5\n",
      "Epoch: [7/10], Step: [201/313],  training loss: 8.3512879550457, Train Acc: 64.4964496449645, Validation Acc: 59.4\n",
      "Epoch: [7/10], Step: [301/313],  training loss: 8.274772119522094, Train Acc: 64.82648264826483, Validation Acc: 57.9\n",
      "Epoch: [8/10], Step: [101/313],  training loss: 8.339311856031419, Train Acc: 64.6064606460646, Validation Acc: 57.1\n",
      "Epoch: [8/10], Step: [201/313],  training loss: 8.13065654039383, Train Acc: 64.9064906490649, Validation Acc: 57.3\n",
      "Epoch: [8/10], Step: [301/313],  training loss: 8.318842989206313, Train Acc: 64.37643764376438, Validation Acc: 56.8\n",
      "Epoch: [9/10], Step: [101/313],  training loss: 8.106684136390687, Train Acc: 65.57655765576558, Validation Acc: 57.6\n",
      "Epoch: [9/10], Step: [201/313],  training loss: 8.082161331176758, Train Acc: 64.75647564756476, Validation Acc: 57.5\n",
      "Epoch: [9/10], Step: [301/313],  training loss: 8.10925179719925, Train Acc: 66.1066106610661, Validation Acc: 58.7\n",
      "Epoch: [10/10], Step: [101/313],  training loss: 7.898031520843506, Train Acc: 65.90659065906591, Validation Acc: 57.8\n",
      "Epoch: [10/10], Step: [201/313],  training loss: 7.909221357107162, Train Acc: 67.42674267426743, Validation Acc: 57.8\n",
      "Epoch: [10/10], Step: [301/313],  training loss: 8.018322682380676, Train Acc: 67.50675067506751, Validation Acc: 58.4\n"
     ]
    }
   ],
   "source": [
    "# In[12]:\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1,data2, lengths1, lengths2, labels in loader:\n",
    "        data_batch1,data_batch2, lengths_batch1,lengths_batch2, label_batch = data1,data2, lengths1, lengths2,labels\n",
    "        outputs = F.softmax(model(data_batch1,data_batch2, lengths_batch1, lengths_batch2), dim=1)\n",
    "        #outputs = model(data_batch1,data_batch2, lengths_batch1, lengths_batch2)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "vocab_size=len(id2words)\n",
    "\n",
    "\n",
    "    \n",
    "model = RNN(emb_size=300, hidden_size=200, num_layers=1, num_classes=3,linear_size =64).cuda()\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (data1,data2, lengths1,lengths2, labels) in enumerate(train_loader):\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1,data2, lengths1,lengths2)\n",
    "        #print(outputs.size())\n",
    "        #print(labels.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}],  training loss: {}, Train Acc: {}, Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader),  running_loss/10, train_acc, val_acc))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
